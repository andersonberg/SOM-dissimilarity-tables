%Apresentao ADS - CIn
%Anderson Berg - absd@cin.ufpe.br

\documentclass[pdf,t]{beamer}
\usetheme{CinArthur2}
%\usecolortheme{Beaver}

\usepackage{time}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage[brazil]{babel}
\usepackage{longtable}
\usepackage{amssymb,amsmath}
\usepackage{palatino}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{enumerate}

\setbeamertemplate{itemize item}{$\bullet$}
\setbeamertemplate{itemize subitem}{$\checkmark$}
\setbeamertemplate{itemize subsubitem}{$\star$}

\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\norm}[1]{\lVert#1\rVert}

\begin{document}

% Informacoes para a Capa
\title[Adaptive batch SOM for multiple dissimilarity data tables]{Adaptive batch SOM for multiple dissimilarity data tables}%
%\subtitle {Uma Abordagem Simbólica}
\author[Anderson Dantas, Francisco de Carvalho]{ \Large{Anderson Dantas \and \\ Francisco de Carvalho}\\
\footnotesize{\{absd, fatc\}@cin.ufpe.br}\\
%\author[Francisco de Carvalho]{ \Large{Francisco de Carvalho}\\
%\footnotesize{fatc@cin.ufpe.br}\\
\vspace*{0.25cm}
\includegraphics[height=2.0cm]{./figuras/ufpe}\\
\vspace*{0.5cm}
\includegraphics[height=1.4cm]{./figuras/logo-cin}
\date{\today}
}

% Capa
\frame{
  \titlepage
}

% Indice
%\AtBeginSection[]
%{
%  \begin{frame}
%    \frametitle{Índice}
%    \scriptsize
%    \tableofcontents[currentsection,hideothersubsections]
%    \normalsize
% \end{frame}
%}

\begin{frame}[c]
\frametitle{Introduction}
\begin{itemize}
\item Clustering methods \textbf{organize} a set of items into clusters 
\item Items within a given cluster have a high degree of \textbf{similarity}
\item \textbf{Feature} data (vector)
\item \textbf{Relational} data (relationship)
\end{itemize}
\end{frame}

\begin{frame}[c]
\frametitle{Objective}
\begin{center}
Hard clustering algorithm that is able to \textbf{partition} objects taking into account \textbf{simultaneously} their relational descriptions given by \textbf{multiple} dissimilarity matrices
\end{center}
\end{frame}

\begin{frame}[c]
\frametitle{Objective}
The algorithm is designed to give:
\begin{itemize}
\item Partition
\item Prototype for each cluster
\item \textbf{Relevance weight}
\end{itemize}
\end{frame}

%\begin{frame}[c]
%\frametitle{Batch Self-Organizing Map Algorithm for dissimilarity data}
%\begin{center}
%Iterative two step algorithm: \\
%Affectation and representation steps\\
%\end{center}
%\end{frame}
%
%\begin{frame}[c]
%\frametitle{Cost function}
%\begin{center}
%Extension of the k-means cost function
%$$J = \sum_{i = 1}^n \sum_{r = 1}^m K^T(\delta(f(e_i),r)) d(e_i, g_r)$$
%$$\lim_{|x| \to \infty} K(x) = 0$$
%\end{center}
%\end{frame}
%
%\begin{frame}[c]
%\frametitle{Generalised distance}
%$$d^T(e_i, g_{f(e_i)}) = \sum_{r=1}^m K^T(\delta(f^T(e_i), r))d(e_i, g_r)$$
%\end{frame}
%
%\begin{frame}[c]
%\frametitle{Affectation step}
%\begin{center}
%Function $f$ associates $e_i$ to the "closest" neuron
%$$
%c = f^T(e_i) = arg \min_{1 \leq r \leq m} d^T(e_i, g_r)
%$$
%\end{center}
%\end{frame}
%
%\begin{frame}[c]
%\frametitle{Representation step}
%\begin{center}
%New prototypes are selected
%$$
%g^*_r = arg \min_{e \in E} \sum_{i=1}^n K^T (\delta(f^T(e_i),r)) d^T(e_i, e_r)
%$$
%\end{center}
%\end{frame}


%\section{Adaptive batch SOM for data based on multiple dissimilarity matrices}

\begin{frame}[c]
\frametitle{Cost function}
\begin{center}
$$
J = \sum_{e_i \in E} \sum_{l=1}^c K^T(\delta(\chi(e_i),l)) D_{\mbox{\boldmath$\lambda$\unboldmath$_l$}}(e_i,G_l) 
$$
\end{center}
\begin{itemize}
\item $D_{\mbox{\boldmath$\lambda$\unboldmath$_l$}}$ is the dissimilarity between $e_i$ and $G_l$ parameterized by relevance weight vector \boldmath$\lambda$\unboldmath$_l \, = (\lambda_{l1}, \ldots, \lambda_{lp})$ 
\item Neighbourhood through kernel functions $K$, parameterized by $T$ defines the influence region around each neuron
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Matching function}
\begin{center}
$$
D_{\mbox{\boldmath$\lambda$\unboldmath$_l$}}(e_i,G_l) = \sum_{j=1}^p \lambda_{lj} D_j(e_i,G_l) = \sum_{j=1}^p \lambda_{lj} d_j(e_i,G_l)
$$
\end{center}
\begin{itemize}
\item To compare clusters and theis prototypes using a different matching measure
\item $d_j(e_i,G_l)$ is the local dissimilarity between an example and the cluster prototype
\end{itemize}
\end{frame}

\begin{frame}[c]
\frametitle{Adaptive B-SOM for data based on multiple dissimilarity matrices}
\begin{center}
Iterative three-step algorithm\\
Representation, weighting and affectation\\
\end{center}
\end{frame}

\begin{frame}[c]
\frametitle{Representation step: computation of the best prototypes}
\begin{center}
Compute the prototype $G_l^{(t)} = G^{*} \in E^{(q)}$ of cluster $P_l^{(t-1)}$ ($\, l = 1, \ldots, c$) according to:
$$
G^{*} = argmin_{e \in E^{(q)}} \sum_{e_i \in E} K^T(\delta(\chi^{(t-1)}(e_i),l)) \sum_{j=1}^p 
\lambda_{lj}^{(t-1)} d_j(e_i,G_l)
$$
\end{center}
\end{frame}

\begin{frame}[c]
\frametitle{Weighting step: definition of the best vectors of weights}
\begin{center}

The vectors of weights \mbox{\boldmath$\lambda$\unboldmath$_l$}$^{(t)}=(\lambda_{l1}^{(t)},\ldots,\lambda_{lp}^{(t)}) \, (l=1,\ldots,c)$, under $\lambda^{(t)}_{lj} > 0$ and $\prod_{j=1}^{p} \lambda^{(t)}_{lj} = 1$, have their weights $\lambda^{(t)}_{lj} \, (j=1,\ldots,p)$ calculated according to:

$$
\lambda^{(t)}_{lj} = \frac{\left\{\displaystyle \prod_{h=1}^p \left[\sum_{e_i \in E}  K^T(\delta(\chi^{(t-1)}(e_i),l)) d_h(e_i,G_l)\right]\right\}^{\frac{1}{p}}}{\displaystyle \left[\sum_{e_i \in E}  K^T(\delta(\chi^{(t-1)}(e_i),l)) d_j(e_i,G_l)\right]}
$$
\end{center}
\end{frame}

\begin{frame}[c]
\frametitle{Affectation step: definition of the best partition}
\begin{center}
$$
m = (\chi^{(t)}(e_i))^{(t)} = argmin_{1 \leq r \leq c} \sum_{l=1}^c K^T(\delta(r,l)) \sum_{j=1}^p \lambda_{lj}^{(t)} d_j(e_i,G_l)
$$
\end{center}
\end{frame}


\begin{frame}[c]
\frametitle{Empirical results}
\begin{center}

\begin{itemize}
\item Databases from the UCI Machine Learning Repository
\item Measures: corrected Rand index (CR), F-measure, overall error rate of classification (OERC)
\item Confusion matrix
\item Data sets described by a matrix of objects x real-valued attributes
%\item All attributes x single attibutes
\end{itemize}

\end{center}
\end{frame}

%\begin{frame}[c]
%\frametitle{Empirical results}
%Parameters
%\begin{itemize}
%\item Topology: 2x5; 
%\item $T_{min}$: 0.3; 
%\item $T_{max}$: 3.0; 
%\item $N_{iter}$: 500
%\end{itemize}
%\end{frame}

\begin{frame}[c]
\frametitle{Wine dataset}
{\scriptsize
\textbf{Parameters}
\begin{itemize}
\item Topology: 2x5; 
\item $T_{min}$: 0.3; 
\item $T_{max}$: 3.0; 
\item $N_{iter}$: 500
\end{itemize}
}
\begin{table}[h!]
\begin{center}
%\caption{Wine data set: $CR$, $F-measure$, and $OERC$ indexes} \label{wine_index}
{\scriptsize
\begin{tabular}{|c|c|c|} \hline
Indexes & B-SOM & AB-SOM \\ \hline
$CR$ & 0.31 & 0.42 \\ \hline
$F-measure$ & 0.45 & 0.52 \\ \hline
$OERC$ & 27.00\% & 9.00\% \\ \hline

\end{tabular}
}
\end{center}
\end{table}
\end{frame}

\begin{frame}[c]
\frametitle{Confusion matrix}
\begin{table}[!h]
\begin{center}
\scriptsize{
\begin{tabular}{|c|c|c|c||c|}
\hline
Cluster/Class & 1 & 2 & 3 & Majority Class \\ \hline
0,0 & 0 & 5 & 16 & 3 \\ \hline
0,1 & 0 & 1 & 4 & 3\\ \hline
0,2 & 0 & 21 & 0 & 2\\ \hline
0,3 & 2 & 22 & 0 & 2\\ \hline
0,4 & 6 & 10 & 0 & 2\\ \hline \hline
1,0 & 0 & 0 & 15 & 3\\ \hline
1,1 & 0 & 0 & 13 & 3\\ \hline
1,2 & 0 & 9 & 0 & 2\\ \hline
1,3 & 24 & 2 & 0 & 1\\ \hline
1,4 & 27 & 1 & 0 & 1\\ \hline

\end{tabular}
}
\end{center}
\end{table}
\end{frame}

\begin{frame}[c]
\frametitle{Final relevance weight matrix}

\begin{center}
\begin{table}[ht]
\scriptsize{
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Cluster/Matrix & 1 & 2 & 3 & 4 & 5 & 6 \\ \hline
0,0 & \textbf{1.03} & 0.27 & \textbf{1.31} & \textbf{1.07} & 0.48 & 0.90 \\ \hline
0,1 & 0.99 & \textbf{1.05} & 0.55 & \textbf{1.09} & 0.27 & 0.69 \\ \hline
0,2 & \textbf{1.28} & 0.52 & 0.41 & 0.43 & \textbf{1.47} & 0.60  \\ \hline
0,3 & 0.69 & 0.33 & 0.93 & \textbf{2.64} & \textbf{1.32} & 0.48 \\ \hline
0,4 & \textbf{0.77} & \textbf{4.19} & 0.27 & 0.99 & 0.16 & \textbf{1.92} \\ \hline
1,0 & \textbf{1.02} & 0.37 & 0.63 & 0.89 & 0.47 & \textbf{1.68} \\ \hline
1,1 & 0.46 & 0.21 & 0.79 & 0.90 & \textbf{1.85} & 0.49 \\ \hline
1,2 & 0.42 & 0.61 & 0.51 & \textbf{1.35} & 0.16 & \textbf{2.93} \\ \hline
1,3 & 0.77 & 0.55 & 0.69 & 0.16 & 0.29 & \textbf{1.38} \\ \hline
1,4 & 0.48 & \textbf{6.01} & 0.37 & 0.33 & 0.61 & \textbf{1.64} \\ \hline

\end{tabular}
}
\end{table}
\end{center}
\end{frame}

\begin{frame}[c]
\frametitle{Conclusions}

\begin{itemize}
\item The algorithm is able to partition objects taking into account their relational description given by multiple dissimilarity matrices.
\item Learn relevance weights that change at each iteration and are different from one cluster to another.
\item Performed better than non-adaptive model for most data bases.
\end{itemize}
\end{frame}

%\begin{frame}[c]
%\frametitle{The B-SOM Algorithm}
%\begin{enumerate}
%\item Initialization
%	\begin{itemize}
%	\item Fix: $m$, $\delta$, $K^T$, $N_{iter}$, $T_{min}$, $T_{max}$, $T \leftarrow T_{max}$, $t \leftarrow 0$
%	\item Randomly select $m$ distinct prototypes
%	\item Set the map $L(m, G^0)$
%	\item Assign each object $e_i$ to the closest neuron (cluster)
%	\end{itemize}
%\end{enumerate}
%
%\end{frame}



%\begin{frame}[c]
%\frametitle{Objetivo}
%\begin{block}{}
%Encontrar uma partição $ P^* = (C_1, ..., C_k)$ de $E$ em $k$ clusters não-vazios e um vetor $L^* = (G_1, ..., G_i, ...G_k)$ tal que $ P^*$ e $L^*$ otimizem o critério:
%\end{block}
%\begin{block}{}
%\begin{center}
%$$\Delta(P^*, L^*) = Min\{\Delta(P,L) / P \in P_k, L \in \Lambda^k\}$$
%\pause
%$$\Delta(P,L) = \sum_{i = 1}^k \sum_{s \in C_i} D(x_s,G_i)$$
%\end{center}
%\end{block}
%\end{frame}
%
%\begin{frame}[c]
%\frametitle{Algoritmo}
%\begin{enumerate}
%\item Inicialização: Inicia de uma partição aleatória $P = (C_1, ..., C_i, ..., C_k)$ ou de um vetor $(G_1, ..., G_i, ..., G_k)$ de $k$ protótipos aleatórios escolhidos entre os elementos de $E$. Neste caso:
%	\begin{itemize}
%	\item $C_i = \emptyset$ para $i = 1, ..., k$
%	\item Para $s = 1$ até $n$ faça:
%		\begin{itemize}
%		\item Atribua $s$ ao cluster $C_l$, $l = argmin_{i = 1, ..., k}D(x_s, G_i)$
%		\item $C_l = C_l \cup \{s\}$
%		\end{itemize}
%	\end{itemize}
%\end{enumerate}
%\end{frame}
%	
%\begin{frame}
%\frametitle{Algoritmo}
%\begin{enumerate}
%\setcounter{enumi}{1}
%\item Etapa de representação: $i = 1$ até $k$, obter o protótipo $G_i$ que minimiza o critério:
%$$f_{C_i}(G) = \sum_{s \in C_i} D(x_s,G), G\in\Lambda$$
%
%\item Etapa de alocação:
%	\begin{itemize}
%	\item $test \leftarrow 0$
%	\item para $s = 1$ até $n$ faça:
%		\begin{itemize}
%		\item Encontre o cluster $C_m$ ao qual $s$ pertence
%		\item Encontre o índice $l$ tal que:  $l = argmin_{i = 1, ..., k}D(x_s, G_i)$
%		\item Se $l \neq m$:\\
%
%			$test \leftarrow 1$\\
%			$C_l = C_l \cup \{s\}$ e $C_m = C_m - \{s\}$\\
%
%		\end{itemize}
%	\end{itemize}
%	
%\item Se $test = 0$ pare, senão vá para 2
%\end{enumerate}
%\end{frame}

\frame{
  \titlepage
}

\end{document} 