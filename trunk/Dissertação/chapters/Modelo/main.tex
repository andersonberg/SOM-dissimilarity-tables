%% M.Sc. Thesis
%% Author: Anderson Berg
%% CIn-UFPE

\chapter{SOM para dados relacionais baseados em múltiplas tabelas de dissimilaridade}

\section{Introdução}

Neste capítulo introduzimos um mapa auto-organizável por lote para dados relacionais baseados em múltiplas matrizes de dissimilaridade. O objetivo do algoritmo apresentado é mapear objetos levando em conta suas descrições relacionais dadas por múltiplas matrizes de dissimilaridade. O modelo aqui proposto é um algoritmo iterativo composto por três etapas, são elas: representação, ponderação e afetação, que serão detalhadas posteriormente. É importante notar que por ser um algoritmo por lote, todo o conjunto de dados é apresentado ao mapa antes que quaisquer alterações sejam realizadas.

O algoritmo SOM por lote para dados relacionais baseados em múltiplas tabelas de dissimilaridade deriva do SOM original por lote e do algoritmo de agrupamento para dados relacionais (\cite{DeCarvalho:2011}). Esta abordagem utiliza diferentes pesos para as matrizes de dissimilaridade, com o objetivo de ponderar a relevância de cada matriz na formação dos agrupamentos. Os pesos mudam a cada iteração do algoritmo, portanto não são definidos absolutamente, além disso, são diferentes de um agrupamento para outro.

\section{Dados relacionais}

Diversos métodos de análise de dados se baseiam em dados que podem ser descritos por valores reais, ou seja, por vetores em um espaço dimensional fixo e finito. Entretanto, muitos dados do mundo real requerem estruturas mais complexas para que sejam representados adequadamente. Textos, por exemplo, não são numéricos e possuem uma estrutura interna complexa que é difícil de representar em um vetor.

Existem basicamente dois tipos de dados em que se baseiam os algoritmos de agrupamento. O mais comum é que se tenha uma matriz contendo valores de medições, onde as linhas da matriz correspondem aos objetos e as colunas às variáveis. Alternativamente, outra maneira de representar os dados é através de uma matriz de dissimilaridade entre os objetos. Tais dissimilaridades podem ser obtidas de diversas formas. Esses tipos de dados são chamado dados relacionais. Agrupamento de dados relacionais é mais utilizado em situações onde os dados não podem ser descritos por características numéricas, também é mais prático quando a distância possui alto grau de complexidade computacional ou quando grupos de objetos similares não podem ser representados eficientemente por um único protótipo.

Muitos algoritmos foram adaptados para analisar dados relacionais. \cite{Kaufman:1987} apresenta uma adaptação do k-means para dados relacionais. Ainda, \cite{Golli:2004} apresenta um modelo de mapa auto-organizável por lote baseado em dados relacionais. O algoritmo apresentado neste trabalho é capaz de analisar dados levando em consideração a dissimilaridade entre observações. Os dados analisados são matrizes de dissimilaridade contendo a relação entre cada um dos objetos presentes na base de dados. Cada matriz representa uma variável da base e a dissimilaridade é calculada por uma função fixa que é a distância euclidiana entre os objetos. Para encontrar uma partição dos elementos, o método descrito leva em consideração simultaneamente a descrição relacional dos dados dada por múltiplas matrizes de dissimilaridade.

\section{Múltiplas matrizes de dissimilaridade}

Em diversas situações, dados relacionais são descritos por múltiplas tabelas de dissimilaridade. Como apontado por \cite{Frigui:2007} muitas aplicações podem se beneficiar de algoritmos de agrupamento baseados em múltiplas matrizes de dissimilaridade. Na categorização de imagens, pode-se ter uma matriz com informações de cor, outra matriz com informação de textura e outra com informação de estrutura. 

Porém, diferentes matrizes não são igualmente importantes, algumas podem ser redundantes, outras irrelevantes, ou ainda, podem interferir negativamente na formação dos agrupamentos. Para que haja uma formação adequada dos agrupamentos faz-se necessário o uso de pesos para cada matriz de dissimilaridade, pesos estes que dependem de cada agrupamento. O objetivo da ponderação sobre as matrizes é encontrar graus de relevância e identificar quais características descrevem melhor os dados, tornando o agrupamento mais significativo.

A ponderação de características deriva da seleção de características e tem sido um tópico de pesquisa importante em algoritmo de aprendizado não-supervisionado. Em \cite{wang:2008}, os autores introduzem um algoritmo fuzzy k-means que tem a vantagem de trabalhar com ponderação para objetos e variáveis simultaneamente. \cite{grozavu:2009} desenvolveu dois modelos usando mapas auto-organizáveis (SOM), que realizam simultaneamente agrupamento e ponderação de variáveis. \cite{Frigui:2007} propõe um algoritmo de agrupamento baseados em múltiplas tabelas de dissimilaridade (CARD) que calcula pesos relacionados à relevância de cada matriz de dissimilaridade sobre cada agrupamento. Em outro trabalho, \cite{frigui:2004} apresenta uma abordagem que realiza agrupamento e ponderação de variáveis simultaneamente.

O modelo aqui apresentado utiliza diferentes pesos adaptativos para cada matriz de dissimilaridades. Esses pesos mudam a cada iteração do algoritmo e, além disso, são diferentes de um agrupamento para outro, ou seja, cada matriz possui uma influência diferente sobre a formação de cada agrupamento. O cálculo dos vetores de pesos neste algoritmo foi inspirado pela abordagem utilizada para calcular pesos para cada variável em cada agrupamento no algoritmo de agrupamento dinâmico baseado em distâncias adaptativas (\cite{diday:1977}).

\section{O algoritmo}

Seja $E = \{e_1,\ldots,e_n\}$ o conjunto de $n$ objetos e $p$ matrizes de dissimilaridade \textbf{D}$_j =[d_j(e_i,e_l)] \, (j=1,\ldots,p)$, onde $d_j(e_i,e_l)$ denota a dissimilaridade entra dois objetos $e_i$ e $e_l \, (i,l=1,\ldots,n)$ na matriz de dissimilaridade \textbf{D}$_j$.

Uma característica importante do modelo introduzido é que este assume que o protótipo $G_k$ do agrupamento $C_k$ é um subconjunto de cardinalidade fixa $1 \leq q << n$ do conjunto $E$ (por questão de simplicidade, geralmente $q = 1$), isto é, $G_k \in E^{(q)} = \{A \subset E: |A| = q\}$.

O algoritmo busca minimizar uma função objetivo dada por:

\begin{equation}
J = \sum_{e_i \in E} \sum_{l=1}^c K^T(\delta(\chi(e_i),l)) D_{\mbox{\boldmath$\lambda$\unboldmath$_l$}}(e_i,G_l) 
\end{equation}

\noindent onde $D_{\mbox{\boldmath$\lambda$\unboldmath$_l$}}$ é a dissimilaridade global entre um objeto $e_i \in P_l$ e o protótipo do agrupamento $G_l \in E^{(q)}$, parametrizada pelo vetor de pesos \boldmath$\lambda$\unboldmath$_l \, = (\lambda_{l1}, \ldots, \lambda_{lp})$ das matrizes de dissimilaridade \textbf{D}$_j$ no agrupamento $P_l \, (l=1,\ldots,c)$.

De acordo com a função de alocação, existem diferentes algoritmos SOM por lote. Neste trabalho consideramos funções de alocação com pesos para cada matriz de dissimilaridade, sendo estes pesos estimados localmente ou globalmente.

Através da função de alocação com pesos estimados localmente, é possível comparar agrupamentos e seus protótipos usando diferentes medidas associadas a cada agrupamento que muda a cada iteração, isto é, a distância não é determinada absolutamente e é diferente de um agrupamento para outro.

A função de alocação parametrizada pelo vetor de pesos \boldmath$\lambda$\unboldmath$_l \, = (\lambda_{l1}, \ldots, \lambda_{lp})$, onde $\lambda_{lj} > 0$ e $\prod_{j=1}^p \lambda_{lj} = 1$ e associada ao agrupamento $P_l \, (l=1,\ldots,c)$ é definida pela seguinte expressão:

\begin{equation}\label{aloc}
D_{\mbox{\boldmath$\lambda$\unboldmath$_l$}}(e_i,G_l) = \sum_{j=1}^p \lambda_{lj} D_j(e_i,G_l) = \sum_{j=1}^p \lambda_{lj} \sum_{e \in G_l} d_j(e_i,e)
\end{equation}

\noindent onde $D_j(e_i,G_l) = \sum_{e \in G_l} d_j(e_i,e)$ representa a dissimilaridade local entre um exemplo $e_i \in P_l$ e o protótipo do agrupamento $G_l \in E^{(q)}$ na matriz \textbf{D}$_j  \, (j=1,\ldots,p)$.

O princípio da função de alocação definida por um vetor de pesos estimado globalmente para todos os agrupamentos é que há uma distância para comparar os agrupamentos e seus protótipos, distância esta que muda a cada iteração, mas é a mesma para todos os agrupamentos.

A função de alocação parametrizada pelo vetor de pesos boldmath$\lambda$\unboldmath$_l$ \, =\boldmath$\lambda$\unboldmath$ \, = (\lambda_{1}, \ldots, \lambda_{p}) \, (l=1,\ldots,c)$, onde $\lambda_{j} > 0$ and $\prod_{j=1}^p \lambda_{j} = 1$, é expressa da seguinte forma:

\begin{equation}\label{aloc-global}
D_{\mbox{\boldmath$\lambda$\unboldmath}}(e_i,G_l) = \sum_{j=1}^p \lambda_{j} D_j(e_i,G_l) = \sum_{j=1}^p \lambda_{j} \sum_{e \in G_l} d_j(e_i,e)
\end{equation} 

\noindent onde $D_j(e_i,G_l)$, novamente representa a dissimilaridade local entre um exemplo $e_i \in P_l$ e o protótipo do agrupamento $G_l \in E^{(q)}$ na matriz \textbf{D}$_j  \, (j=1,\ldots,p)$.

Quando $T$ é fixo, a minimização da função $J$ é realizada iterativamente em três etapas: representação, ponderação e afetação.

\subsection{Etapa de representação: determinação dos melhores protótipos}

Na etapa de representação, a partição $P^{(t-1)}=(P_1^{(t-1)},\ldots,P_c^{(t-1)})$ e os vetores de pesos \mbox{\boldmath$\lambda$\unboldmath$_l$}$^{(t-1)} \, (r=1,\ldots, c)$ são fixos. A função objetivo $J$ é minimizada de acordo com os protótipos.

\begin{proposition}\label{prop:prototype}
O protótipo é atualizado de acordo com a função de alocação usada:
\begin{enumerate}
\item Se a função de alocação é definida pela equação \ref{aloc}, calcule o protótipo $G_l^{(t)} = G^{*} \in E^{(q)}$ do agrupamento $P_l^{(t-1)}$ ($\, l = 1, \ldots, c$) segundo a expressão:

\begin{equation} \label{prototipo}
G^{*} = argmin_{G \in E^{(q)}} \sum_{e_i \in E} K^T(\delta(\chi^{(t-1)}(e_i),l)) \sum_{j=1}^p 
\lambda_{lj}^{(t-1)} \sum_{e \in G} d_j(e_i,e)
\end{equation}

\item Se a função de alocação é definida pela equação \ref{aloc-global}, calcule o protótipo $G_l^{(t)} = G^{*} \in E^{(q)}$ do agrupamento $P_l^{(t-1)}$ ($\, r = 1, \ldots, c$) de acordo com a equação seguinte:
\begin{equation} \label{prototipo-global}
G^{*} = argmin_{G \in E^{(q)}} \sum_{e_i \in E} K^T(\delta(\chi^{(t-1)}(e_i),l)) \sum_{j=1}^p 
\lambda_{j}^{(t-1)} \sum_{e \in G} d_j(e_i,e)
\end{equation}

\end{enumerate}

\end{proposition}

\subsection{Etapa de ponderação: definição dos melhores vetores de pesos}

Durante a etapa de ponderação, a partição $P^{(t-1)}=(P_1^{(t-1)},\ldots,P_c^{(t-1)})$ e os 
protótipos $G_l^{(t)} \in E^{(q)} \, (l=1,\ldots, c)$ são mantidos fixos. A função objetivo $J$ é minimizada de acordo com os vetores de pesos.

\begin{proposition}\label{prop:distance}
Os vetores de pesos são atualizados de acordo com a função de alocação utilizada:

\begin{enumerate}
\item Se a função de alocação é definida pela equação \ref{aloc}, os vetores de pesos \mbox{\boldmath$\lambda$\unboldmath$_l$}$^{(t)}=(\lambda_{l1}^{(t)},\ldots,\lambda_{lp}^{(t)}) \, (l=1,\ldots,c)$, com $\lambda^{(t)}_{lj} > 0$ e $\prod_{j=1}^{p} \lambda^{(t)}_{lj} = 1$, têm seus pesos $\lambda^{(t)}_{lj} \, (j=1,\ldots,p)$ calculados de acordo com a equação:

\begin{equation} \label{weight-1}
\lambda^{(t)}_{lj} = \frac{\left\{\displaystyle \prod_{h=1}^p \left[\sum_{e_i \in E}  K^T(\delta(\chi^{(t-1)}(e_i),l)) \sum_{e \in G_l^{(t)}} d_h(e_i,e)\right]\right\}^{\frac{1}{p}}}{\displaystyle \left[\sum_{e_i \in E}  K^T(\delta(\chi^{(t-1)}(e_i),l)) \sum_{e \in G_l^{(t)}} d_j(e_i,e)\right]}
\end{equation}

\item Se a função de alocação é definida pela equação \ref{aloc-global}, o vetor de pesos \mbox{\boldmath$\lambda$\unboldmath}$^{(t)}=(\lambda_{1}^{(t)},\ldots,\lambda_{p}^{(t)})$ , com $\lambda^{(t)}_{j} > 0$ e $\prod_{j=1}^{p} \lambda^{(t)}_{j} = 1$, tem seus pesos $\lambda^{(t)}_{j} \, (j=1,\ldots,p)$ calculados segundo a equação:

\begin{equation} \label{weight-global}
\lambda^{(t)}_{j} = \frac{\left\{\displaystyle \prod_{h=1}^p\left[\sum_{r=1}^c\left(\sum_{e_i \in E}  K^T(\delta(\chi^{(t-1)}(e_i),l)) \sum_{e \in G_l^{(t)}} d_h(e_i,e)]\right)\right]\right\}^{\frac{1}{p}}}{\displaystyle \sum_{r=1}^c \left[\sum_{e_i \in E}  K^T(\delta(\chi^{(t-1)}(e_i),l)) \sum_{e \in G_l^{(t)}} d_j(e_i,e)\right]}
\end{equation}

\end{enumerate}
\end{proposition}

%\textbf{Prova}
%
%Vamos demonstrar a prova para o caso onde a função de alocação é definida pela equação \ref{aloc}. A prova para o outro caso pode ser feita de maneira similar.
%
%Como a partição $P=(P_1,\ldots,P_c)$ de $E$ em $c$ agrupamentos e os protótipos $G_1,\ldots,G_c$ são fixos, pode-se reescrever a função objetivo $J$:
%
%\[
%J(\mbox{\boldmath$\lambda$\unboldmath$_1$},\ldots,\mbox{\boldmath$\lambda$\unboldmath$_c$}) = \sum_{l=1}^c J_l(\mbox{\boldmath$\lambda$\unboldmath$_l$})
%\]
%
%com
%
%\[
%J_l(\mbox{\boldmath$\lambda$\unboldmath$_l$}) = J_l(\lambda_{l1}, \ldots, \lambda_{lp})= \sum_{j=1}^p \lambda_{lj} \, J_{lj}
%\mbox{ where } J_{lj} = \sum_{e_i \in E} K^T(\delta(\chi(e_i),l)) \sum_{e \in G_l} d_j(e_i,e)
%\]
%
%Seja $g(\lambda_{l1},\ldots,\lambda_{lp}) = \lambda_{l1} \times \ldots \times \lambda_{lp} - 1$. Pode-se determinar os extremos de $J_l(\lambda_{l1},\ldots,\lambda_{lp})$ com a restrição $g(\lambda_{l1},\ldots,\lambda_{lp}) =0$. 
%Do método Lagrange multiplier, e após some algebra, it follows that (for $j = 1, \ldots, p$)
%
%\[ \label{meth1:lamb}
%\lambda_{lj} = \frac{
%\left(\prod_{h=1}^p J_{lh}\right)^{1/p}}{J_{lj}
%}=
%\frac{
%\left\{\prod_{h=1}^p \left( \sum_{e_i \in E} K^T(\delta(\chi(e_i),l)) \sum_{e \in G_l} d_h(e_i,e)
%\right)
%\right\}^{\frac{1}{p}}
%}
%{\sum_{e_i \in E} K^T(\delta(\chi(e_i),l)) \sum_{e \in G_l} d_j(e_i,e)
%}
%\]
%
%Assim, um valor extremo de $J_l$ é alcançado quando $J_l(\lambda_{l1},\ldots,\lambda_{lp})
%= p \, \{J_{l1} \times \ldots \times J_{lp} \}^{1/p}$.
%Como $J_l(1,\ldots,1)=\sum_{j=1}^p J_{lj}=J_{l1}+\ldots+J_{lp}$ e é sabido que a média aritmética é maior do que a média geométrica, isto é, $\frac{1}{p} \left(J_{l1}+\ldots+J_{lp}\right)> \left\{J_{l1}
%\times \ldots \times J_{lp}\right\}^{1/p}$ (the equality holds only
%if $J_{l1}= \ldots = J_{lp}$), pode-se concluir que este extremo é um valor mínimo.

\subsection{Etapa de afetação: definição da melhor partição}

Durante a etapa de afetação os protótipos $G_l^{(t)} \in E^{(q)} \, (l=1,\ldots, c)$ e os vetores de pesos \mbox{\boldmath$\lambda$\unboldmath$_l$}$^{(t-1)} \, (r=1,\ldots, c)$ são mantidos fixos. A função objetivo $J$ é minimizada de acordo com a função de afetação.

\begin{proposition}\label{prop:partition}
Cada exemplo $e_i \in E$ é alocado ao neurônio mais próximo de acordo com a função de alocação utilizada:

\begin{enumerate}
\item Se a função de alocação é definida pela equação (\ref{aloc}), alocar o exemplo $e_i \in E$ no agrupamento $C_m$ segundo a equação:
\begin{equation} \label{part-1}
m = (\chi^{(t)}(e_i))^{(t)} = argmin_{1 \leq r \leq c} \sum_{l=1}^c K^T(\delta(r,l)) \sum_{j=1}^p \lambda_{lj}^{(t)} \sum_{e \in G_l^{(t)}} d_j(e_i,e)
\end{equation}

\item Se a função de alocação é definida pela equação (\ref{aloc-global}), alocar o exemplo $e_i \in E$ no agrupamento $C_m$ segundo a expressão:
\begin{equation} \label{part-3}
m = (\chi^{(t)}(e_i))^{(t)} = argmin_{1 \leq r \leq c} \sum_{l=1}^c K^T(\delta(r,l)) \sum_{j=1}^p \lambda_{j}^{(t)} \sum_{e \in G_l^{(t)}} d_j(e_i,e)
\end{equation}

\end{enumerate}
\end{proposition}

\subsection{O algoritmo}

O algoritmo SOM em lote para dados relacionais baseados em múltiplas matrizes de dissimilaridade pode ser resumido como segue:

\begin{enumerate}
%\item {\bf Algorithm}
\item Inicialização
\\Fixe o número $c$ de agrupamentos; 
\\Fixe a cardinalidade $1 \leq q << n$ dos protótipos $G_l \, (l=1,\ldots,c)$;
\\Fixe $\delta$; Fixe a função kernel $K$
\\Fixe o número de iterações $N_{iter}$
\\Fixe $T_{min}$, $T_{max}$; Determine $T\leftarrow T_{max}$; Determine $t\leftarrow 0$;
\\Selecione aleatoriamente $c$ protótipos distintos $G_l^{(0)} \in E^{(q)} \, (l=1,\ldots,c)$;
\\Determine \mbox{\boldmath$\lambda$\unboldmath$_l$}$^{(0)}=(1,\ldots,1) \, (l=1,\ldots,c)$;
\\Determine o mapa $L(c,\textbf{G}^{0})$, onde $\textbf{G}^{0}=(G_1^{(0)}, \ldots, G_c^{(0)})$
\\Aloque cada objeto $e_i$ ao protótipo mais próximo para obter a partição $P^{(0)}=(P_1^{(0)},\ldots,P_c^{(0)})$ de acordo com as equações (\ref{part-1}) e (\ref{part-3})
\item {\it Etapa de representação: cálculo dos melhores protótipos}.
\\ Determine $t=t+1$;
\\Calcule $T=T_{max} (\frac{T_{min}}{T_{max}})^{\frac{t}{N_{iter}-1}}$
\\A partição $P^{(t-1)}=(P_1^{(t-1)},\ldots,P_c^{(t-1)})$ e \mbox{\boldmath$\lambda$\unboldmath$_l$}$^{(t-1)} \, (l=1,\ldots, c)$ são mantidos fixos. 
\\Calcule o protótipo $G_l^{(t)} = G^{*} \in E^{(q)}$ do agrupamento $P_l^{(t-1)}$ ($\, l = 1, \ldots, c$) de acordo com as equações (\ref{prototipo}) e (\ref{prototipo-global})
\item {\it Etapa de ponderação: cálculo dos melhores pesos}.
\\A partição $P^{(t-1)}=(P_1^{(t-1)},\ldots,P_c^{(t-1)})$ e os 
protótipos $G_l^{(t)} \in E^{(q)} \, (l=1,\ldots, c)$ são mantidos fixos. 
\\Calcule os vetores de pesos \mbox{\boldmath$\lambda$\unboldmath$_l$} \, $(l=1,\ldots,c)$ de acordo com as equações (\ref{weight-1}) e (\ref{weight-global})
\item[4)] {\it Etapa de afetação: definição da melhor partição}.
 \\ Os protótipos $G_l^{(t)} \in E^{(q)} \, (l=1,\ldots, c)$ e \mbox{\boldmath$\lambda$\unboldmath$_l$}$^{(t)} \, (l=1,\ldots, c)$ são mantidos fixos
 \\ 	$P^{(t)} \leftarrow P^{(t-1)}$
  \begin{tabbing}
  para \= $i = 1$ até $n$ faça \\
  		\>encontre o agrupamento $C_{m^*}^{(t)}$ ao qual $e_i$ pertence
  		\\
      \>encontre \= o agrupamento vencedor $C_{m}^{(t)}$ segundo as equações (\ref{part-1}) e (\ref{part-3}) 
      \\
      \>se $m^* \neq m$ \\
      \>     \>$C_{m}^{(t)} \leftarrow C_{m}^{(t)} \cup{\{e_i\}}$ \\
      \>     \>$C_{m^*}^{(t)} \leftarrow C_m^{(t)} \setminus{\{e_i\}}$\\
  \end{tabbing}
  \item[4)] {\it Critério de parada}.
  \\Se $T = T_{min}$ (ou se $t=N_{iter}-1$) então PARE; senão vá para 2 (Etapa de representação).
\end{enumerate}