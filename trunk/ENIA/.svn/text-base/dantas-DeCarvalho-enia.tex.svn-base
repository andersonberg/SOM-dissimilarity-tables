\documentclass[12pt]{article}

\usepackage{sbc-template}

\usepackage{graphicx,url}
\usepackage{amsfonts}
\usepackage[brazil]{babel}   
\usepackage[latin1]{inputenc}  

     
\sloppy

\title{Agrupamento de múltiplas tabelas de dissimilaridade\\ baseado em SOM com pesos adaptativos}

\author{Anderson B. S. Dantas\inst{1}, Francisco de A. T. de Carvalho\inst{1} }


\address{Centro de Informática -- Universidade Federal de Pernambuco (CIn/UFPE)\\
  Av. Prof. Luiz Freire, s/n - Cidade Universitária, CEP 50740-540, Recife -- PE -- Brazil
  \email{\{absd,fatc\}@cin.ufpe.br}
}

\begin{document} 

\maketitle

\begin{abstract}

\end{abstract}
     
\begin{resumo}

\end{resumo}


\section{Introdução}


\section{Self-organizing map (SOM)} \label{sec:som}
A classe de métodos chamados \textit{self-organizing maps} \cite{Kohonen:1990} (mapas auto-organizáveis) segue uma série de procedimentos para associar um número finito de vetores de objetos (entradas) com um número finito de pontos representativos, tal que as relações de semelhança entre as entradas são respeitadas por esses pontos. O algoritmo SOM representa uma abordagem de aprendizado não-supervisionado, todas as propriedades de cada cluster são estimados ou aprendidos sem o uso de informação a priori \cite{Murtagh:2010}.

O SOM consiste em um conjunto de neurônios organizados topologicamente em uma grade, geralmente com uma, duas, ou três dimensões, chamada mapa. De modo mais formal, o mapa é descrito por um grafo não-orientado ($C,\Gamma$). $C$ é um conjunto de $m$ neurônios interconectados com topologia definida por $\Gamma$. A estrutura de grafo permite que se defina uma função de distância entre dois neurônios no mapa. Para cada par de neurônios (\textit{c,r}) no mapa, $\delta(c,r)$ é o comprimento do caminho mais curto entre \textit{c} e \textit{r} no grafo \textit{C}. Essa função de distância permite que haja uma relação de vizinhança entre neurônios.

O conceito de vizinhança é incorporado ao algoritmo através do uso de uma função \textit{kernel} $K$ positiva e tal que $\lim_{|x| \to \infty} K(x) = 0$ \cite{Badran:2005}. As distâncias $\delta(c,r)$ entre os neurônios \textit{c} e \textit{r} do mapa permitem a definição da influência relativa dos neurônios sobre os objetos. $K(\delta(c,r))$ quantifica essa influência. Os neurônios e os padrões de entrada são ambos associados a vetores m-dimensionais. O neurônio com melhor correspondência será atualizado, como também os neurônios na vizinhança ao redor deste "vencedor". A região ao redor do neurônio que melhor representa um dado objeto de entrada é modificada para se aproximar mais do objeto apresentado. O resultado é que os neurônios no mapa ficam ordenados, neurônios vizinhos possuem vetores referência (ou protótipos) similares.

Considere um vetor $\mathbf{w}_i (\mathbf{w}_i \in \mathbb{R}^p)$ associado a cada elemento $i$ do mapa. O vetor do elemento $i$ na iteração $t$ é $\mathbf{w}_i^{(t)}$. Atribua $t = 0$.

\textbf{Etapa 0:} Escolha um vetor $\mathbf{x} (\mathbf{x} \in \mathbb{R}^p)$ aleatoriamente do conjunto de entradas.

\textbf{Etapa 1:} Determine o neurônio $c = i$ tal que $\parallel \mathbf{x} - \mathbf{w}_i^{(t)} \parallel$ é mínimo. Onde $\parallel \cdot \parallel$ representa uma medida de dissimilaridade que pode ser expressa em termos de uma função de distância, por exemplo, a distância Euclidiana seria uma escolha plausível.

\textbf{Etapa 2:} Para todos os elementos $i$ do mapa:
\begin{eqnarray*}
\mathbf{w}_i^{(t+1)} &=& \mathbf{w}_i^{(t)} + \alpha^{(t)} (\mathbf{x} - \mathbf{w}_i^{(t)}) \; \mbox{se} \, i \in N_c(t)\\
&=& \mathbf{w}_i^{(t)} \; \mbox{se} \, i \notin N_c(t)
\end{eqnarray*}
onde $\alpha^{(t)}$ é uma pequena taxa usada para controlar a convergência e $N_c(t)$ é a vizinhança do elemento $c$ do mapa.

\textbf{Etapa 3:} Incremente $t$; retorne ao Passo 0, a menos que uma condição de parada seja atingida.

É importante notar que neste tipo de algoritmo somente um indivíduo é apresentado por vez (Etapa 0), os neurônios do mapa se ajustam às entradas a medida em que elas são apresentadas.

\section{Batch self-organizing map aplicado em dados de dissimilaridade}

%\subsection{Algoritmo de treinamento batch}
%
%O algoritmo de treinamento do tipo batch é um algoritmo iterativo onde todo o conjunto de exemplos ($E$) é apresentado ao mapa antes que qualquer ajuste seja realizado. Definimos uma função de afetação $f$ de $\mathbb{R}^p$ em $C$, que associa cada elemento $x_i \in \mathbb{R}^p$ ao neurônio cujo vetor é mais semelhante a $x_i$. Essa função induz uma partição $P = \{P_c; c = 1,\dots, k\}$ do conjunto de indivíduos, onde $P_c = \{x_i \in E; f(x_i) = c\}$. Esta é a chamada etapa de afetação.
%
%Após a etapa de afetação, a etapa de representação é realizada. O algoritmo atualiza os vetores dos neurônios minimizando uma função custo $E(j,W)$

\subsection{SOM batch para dados de dissimilaridade}
O SOM para dados de dissimilaridade conforme proposto por \cite{golli:2004} também é descrito por um grafo ($C,\Gamma$). A principal diferença está nos dados que serão classificados que consistem em um conjunto arbitrário onde uma dissimilaridade é definida.

Cada neurônio $c$ é representado por um protótipo $a_c = x_{j_i}, \: x_{j_i} \in E$. 
%Denotamos $a$ \textit{individuals codebook}, i.e. a lista $a = \{a_c; c = 1,\dots,m\}$ de \textit{individual referents} do mapa.

Uma dissimilaridade $d^T$ de $E \times P(E)$ em $\mathbb{R}$ é definida por:

\begin{equation}
d^T(x_i, a_c) = \sum_{r \in C}K^T(\delta(c,r)) \, d^2(x_i,a_r)
\end{equation}

Tal dissimilaridade é baseada na função kernel positiva $K$, como descrita na seção anterior. $K$ é usada para definir uma família de funções $K^T$ parametrizadas por $T$, onde $K^T(\delta) = K (\frac{\delta}{T})$. O parâmetro $T$ controla o tamanho da vizinhança dos neurônios: se o valor de $T$ for pequeno, poucos neurônios serão atingidos pela vizinhança. Um simples exemplo para $K^T$ é definido por $K^T(\delta) = e^{-\frac{\delta^2}{T^2}}$.

Durante a etapa de aprendizado, a seguinte função custo $E$ é minimizada alternando-se o passo de afetação e o passo de representação:

\begin{equation}
E(f,a) = \sum_{i = 1}^n d^T(x_i, a_{f(x_i)}) = \sum_{i = 1}^n \sum_{r \in C}K^T(\delta(f(x_i),r)) d^2(x_i, a_r)
\end{equation}

Na etapa de afetação, a função $f$ afeta cada indivíduo $x_i$ ao neurônio mais próximo. Em termos da dissimilaridade $d^T$:

\begin{equation}
f(x_i) = arg \min_{c \in C} d^T(x_i, a_c)
\label{affectation_function}
\end{equation}

Na etapa de representação, são selecionados novos protótipos que representam o conjunto de objetos. Esta etapa de otimização pode ser realizada independentemente para cada neurônio. De fato, as seguintes $m$ funções são minimizadas:

\begin{equation}
E_r = \sum_{x_i \in \Omega} K^T(\delta(f(x_i),r)) d^2(x_i, a_r)
\label{E_batch_rossi}
\end{equation}

\subsection{O Algoritmo}

\textbf{Inicialização}: iteração $k = 0$; selecione os protótipos iniciais $a^0$; fixe $T = T_{max}$ e o número total de iterações $N_{iter}$\\
\textbf{\underline{Iteração}}: Na iteração $k$, o conjunto de protótipos da iteração anterior $a^{k-1}$ é conhecido. Calcule o novo valor de $T$:
$$
T = T_{max}\ast (\frac{T_{min}}{T_{max}})^\frac{k}{N_{iter}-1}
$$

\begin{itemize}
\item \textbf{etapa de afetação}: Afetar cada indivíduo $X_i$ ao protótipo como definido na equação \ref{affectation_function}.
\item \textbf{etapa de representação}: determinar os novos protótipos $a^k*$ que minimizam a função $E(f_{a^k},a)$, $a^k_c*$ é definido pela equação \ref{E_batch_rossi}.
\end{itemize}

Repetir \textbf{\underline{Iteração}} até $T = T_{min}$.

\section{Self-organizing map adaptativo para múltiplas tabelas de dissimilaridade}

\subsection{Introdução}

Nesta seção apresentamos um algoritmo baseado no algoritmo SOM para dados de dissimilaridade \cite{golli:2004} e no algoritmo de agrupamento (\textit{clustering}) de múltiplas tabelas de dissimilaridade de \cite{Lechevallier:2010}. O objetivo do modelo apresentado é mapear objetos levando em conta suas descrições relacionais dadas por múltiplas matrizes de dissimilaridade. O algoritmo é capaz de associar dados de entrada com pontos representativos de um mapa formando grupos (\textit{clusters}) de objetos.

Seja $E = \{e_1,\dots,e_n\}$ um conjunto de $n$ objetos e $p$ o número de matrizes de dissimilaridade $\textbf{D}_j = [d_j(e_i,e_l)] (j = 1,\dots,p)$, onde $d_j(e_i,e_l)$ é a dissimilaridade entre os objetos $e_i$ e $e_l (i,l = 1,\dots,n)$ na matriz de dissimilaridades $\textbf{D}_j$. Assuma que o protótipo $g_l$ do \textit{cluster} $C_l$ é um elemento do conjunto de objetos $E$, i.e., $g_l \in E \: \forall l = 1,\dots,L$.

O algoritmo SOM ponderado para múltiplas tabelas de dissimilaridade calcula uma partição $P = (C_1,\dots, C_l)$ de $E$ em $L$ clusters e o respectivo protótipo $g_l$ representando o cluster $C_l$ em $P$ tal que um determinado critério de adequação (função objetivo) que mede  o ajuste entre os clusters e seus repectivos protótipos é localmente otimizado. O critério de adequação é definido como:

\begin{eqnarray}
E &=& \sum_{i=1}^n \sum_{l=1}^c K(\delta(f(e_i),l),T)D_{ \lambda_l}(e_i,g_l) \nonumber\\
&=& \sum_{i=1}^n \sum_{l=1}^c \exp \{- \frac{(\delta((f(e_i)),l))^2}{2T^2} \sum_{j=1}^p \lambda_{lj} d_j(e_i, g_l)
\end{eqnarray}

onde $K$ é uma função kernel positiva, tal que $\lim_{|\delta| \to \infty} K(\delta) = 0$. E $\delta(c,r)$ calcula a distância entre dois neurônios no mapa.
A matriz de pesos $\lambda$ é composta por $L$ vetores de pesos $\lambda_k = (\lambda_k^1,\dots,\lambda_k^j,\dots,\lambda_k^p)$, muda a cada iteração do algoritmo, ou seja, os vetores não possuem valores determinados absolutamente e é diferente para cada matriz de dissimilaridades. O modelo segue três etapas definidas (representação, atualização de pesos e afetação):

\subsection*{\textbf{\textit{Etapa 1: Definição dos melhores protótipos}}}
Nesta etapa, a partição $P^{(t-1)} = (P_1^{(t-1)},\dots,P_c^{(t-1)})$ e $\lambda_l^{(t)} (l = 1,\dots,c)$ são fixos. O protótipo $g_l^{(t)} = g^* \in E$ do cluster $C_l$, que minimiza o critério de adequação $E$ é calculado segundo a equação:

\begin{equation}
g^* = argmin_{e \in E} \sum_{i=1}^n \exp \{- \frac{(\delta((f(e_i))^{(t-1)},l))^2}{2T^2} \sum_{j=1}^p \lambda_{lj}^{(t-1)} d_j(e_i, e)
\label{prototipo}
\end{equation}

\subsection*{\textbf{\textit{Etapa 2: Definição dos melhores pesos}}}
Nesta etapa, a partição $P^{(t-1)} = (P_1^{(t-1)},\dots,P_c^{(t-1)})$ e os protótipos $g_l^{(t)} \in E (l = 1,\dots,c)$ são fixos. O elemento $j$ do vetor de pesos $\lambda_l = (\lambda_k^1,\dots,\lambda_k^p)$, que minimiza o critério de adequação $E$ é calculado segundo a expressão:

\begin{equation}
\lambda_{lj}^{(t)} = \frac{\left\lbrace  \prod_{h=1}^p ( \sum_{i=1}^n \exp\{- \frac{\delta^2(f^{(t-1)}(x_i), l)}{2T^2} \} d_h(e_i,g_l^{(t)})) \right\rbrace  ^ \frac{1}{p} }{\sum_{i=1}^n \exp\{- \frac{\delta^2(f^{(t-1)}(x_i), l)}{2T^2} \} d_j(e_i,g_l^{(t)})}
\label{pesos}
\end{equation}

\subsection*{\textbf{\textit{Etapa 3: Definição da melhor partição}}}
Nesta etapa, os protótipos $g_l^{(t)} \in E (l = 1,\dots,c)$ e $\lambda_l^{(t)} (l = 1,\dots,c)$ são fixos. O cluster $C_r$ que minimiza o critério E é determinado de acordo com a equação:

\begin{equation}
r = (f(e_i))^{(t)} = argmin_{1\leq h\leq c} \sum_{l=1}^c \exp \{- \frac{(\delta(h,l))^2}{2T^2}\} \sum_{j=1}^p \lambda_{lj}^{(t)} d_j(e_i, g_l^{(t)})
\label{cluster}
\end{equation}


%Descomentar as linhas no caso de $G_l$ ser um subconjunto de $E$
%Assumindo que o protótipo $G_l$ do \textit{cluster} $C_l$ é um subconjunto de cardinalidade fixa $1 \leq q \ll n$ do conjunto de objetos $E$, isto é, $G_l \in E^{(q)} = \{A \subset E : |A| = q\}$.
%\begin{equation}
%E = \sum_{i=1}^n \sum_{l=1}^c \exp \{- \frac{(\delta((f(e_i)),l))^2}{2T^2} \sum_{j=1}^p \lambda_{lj} \sum_{e %\in G_l} d_j(e_i, e)
%\end{equation}

\subsection{O algoritmo}

\begin{enumerate}
\item \textit{Inicialização\\}
Fixe o número $c$ de clusters;\\
%Fixe a cardinalidade $1 \leq q \ll n$ dos protótipos $G_l (l = 1,\dots,c)$;\\
Fixe $\delta$;\\
Fixe a função kernel $K$;\\
Fixe o número de iterações $N_{iter}$;\\
Fixe $T_{min}$, $T_{max}$; Atribua $T \leftarrow T_{max}$; Atribua $t \leftarrow 0$;\\
Selecione $c$ protótipos aleatoriamente $g_l^{(0)} \in E(l = 1,\dots,c$;\\
Configure $\lambda_l^{(0)} = (\lambda_{l1}^{(0)},\dots,\lambda_{lp}^{(0)}) = (1,\dots,1)(l = 1,\dots,c)$;\\
Configure o mapa $L(c,\textbf{G}^0)$, onde $\textbf{G}^0 = (g_1^{(0)},\dots,g_c^{(0)})$.\\
Cada objeto $e_i$ é afetado ao protótipo mais próximo com o objetivo de obter a partição $P^{(0)} = (P_1^{(0)},\dots,P_c^{(0)})$ de acordo com o seguinte critério:
\begin{equation}
(f(e_i))^{(0)} = argmin_{1\leq r\leq c}\sum_{l=1}^c \exp \{- \frac{(\delta(r,l))^2}{2T^2}\} \sum_{j=1}^p \lambda_{lj}^{(0)} d_j(e_i, g_l)
\end{equation}

\item \textit{Passo 1: Representação}\\
Atribua $t = t + 1$;\\
Atribua $T = T_{max} (\frac{T_{min}}{T_{max}})^\frac{t}{N_{iter} - 1}$\\
A partição $P^{(t-1)} = (P_1^{(t-1)},\dots,P_c^{(t-1)})$ e $\lambda_l^{(t)} (l = 1,\dots,c)$ são fixos.\\
Calcule o protótipo $g_l^{(t)} = g^* \in E$ do cluster $P_l^{(t-1)} (l = 1,\dots,c)$ de acordo com a equação \ref{prototipo}.


\item Passo 2: Atualização dos pesos\\
A partição $P^{(t-1)} = (P_1^{(t-1)},\dots,P_c^{(t-1)})$ e os protótipos $g_l^{(t)} \in E (l = 1,\dots,c)$ são fixos.\\
Atualize o vetor de pesos de acordo com a equação \ref{pesos}.


\item \textit{Passo 3: Afetação}\\
Os protótipos $g_l^{(t)} \in E (l = 1,\dots,c)$ e $\lambda_l^{(t)} (l = 1,\dots,c)$ são fixos\\ 
$P^{(t)} \leftarrow P^{(t-1)}$\\
para $i=1$ até $n$ faça:\\
encontre o cluster $C_m^{(t)}$ ao qual o objeto $e_i$ pertence\\
encontre o cluster vencedor $C_r^{(t)}$, onde $r$ é obtido pela equação \ref{cluster}\\

se $r \neq m:\\$
$C_r^{(t)} \leftarrow C_r^{(t)} \cup \{e_i\}$\\
$C_m^{(t)} \leftarrow C_m^{(t)} \setminus \{e_i\}$\\

\item \textit{Critério de parada}\\
Se $T == T_{min}$ então PARE; senão volte ao passo 1

\end{enumerate}

\section{Experimentos e resultados}

Para provar a utilidade do algoritmo proposto, foram realizados experimentos utilizando bases de dados obtidas do repositório UCI Machine Learning Repository \cite{Frank:2010}. Com a finalidade de avaliar os resultados da clusterização realizada pelo método proposto neste trabalho foi considerado um índice externo: o índice de Rand corrigido (CR) \cite{Hubert:1985}, a medida F-\textit{measure} \cite{Rijsbergen:1979} e a taxa de erro global de classificação (OERC - overall error rate of classification) \cite{Breiman:1984}.

O índice CR avalia o grau de similaridade entre a partição a priori e a partição fornecida pelo algoritmo de agrupamento. Além disso, o índice CR não é sensível ao número de classes nas partições ou à distribuição dos itens dentro dos clusters. O índice CR pode assumir valores no intervalo [-1,1], onde o valor 1 indica perfeita combinação entre as partições, valores perto de 0 ou negativos indicam pouca semelhança. O índice F-\textit{measure} entre a partição a priori e a partição obtida pelo algoritmo de agrupamento assume valores no intervalo [0,1], em que 1 indica perfeita combinação entre as partições. O índice OERC mede a habilidade de um algoritmo de agrupamento encontrar classes a priori presentes na base de dados.

Alguns parâmetros são considerados para a realização dos experimentos. São eles: topologia do mapa, o valor de $T_{min}$, $T_{max}$ e o número de iterações $N_{iter}$. A topologia do mapa define o número máximo de clusters que serão formados. Como os protótipos de cada cluster são os próprios objetos de entrada, o número máximo de clusters deve ser um número menor do que a quantidade de indivíduos presentes na base de dados. Todos os parâmetros foram definidos empiricamente.

\subsection{Base de dados Iris}
Esta base de dados consiste em três tipos (classes) de planta íris: iris setosa, iris versicolor e iris virginica. Cada uma das classes possui 50 exemplos. Cada instância é descrita por quatro atributos, que são o comprimento (length) e a largura (width) da sépala (sepal) e da pétala (petal), em centímetros.

Os algoritmos SOM foram aplicados nesta base de dados com os parâmetros apresentados na tabela \ref{param_iris}. As tabelas \ref{iris_batch} e \ref{iris_adaptativo} mostram a matriz de confusão obtidas pelos algoritmos batch SOM e batch SOM adaptativo para múltiplas tabelas de dissimilaridade.

\begin{table}[!h]
\begin{center}
\caption{Parâmetros utilizados nos experimentos com a base iris}
\begin{tabular}{|c|c|}
\hline
Parâmetros & Valor\\
\hline
Topologia & 2x5\\ \hline
$T_{min}$ & 0,02\\ \hline
$T_{max}$ & 2,0\\ \hline
$N_{iter}$ & 300\\ \hline
\end{tabular}
\label{param_iris}
\end{center}
\end{table}

\begin{table}[!h]
\begin{center}
\caption{Base de dados iris: Matriz de confusão obtida pelo algoritmo batch SOM}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline
Classe/Cluster & 0,0 & 0,1 & 0,2 & 0,3 & 0,4 & 1,0 & 1,1 & 1,2 & 1,3 & 1,4 \\ \hline
1 & 13 & 17 & 4 & 0 & 0 & 16 & 0 & 0 & 0 & 0 \\ \hline
2 & 0 & 0 & 0 & 12 & 15 & 0 & 19 & 0 & 0 & 4 \\ \hline
3 & 0 & 0 & 0 & 1 & 2 & 0 & 0 & 3 & 26 & 18 \\ \hline
\end{tabular}
\label{iris_batch}
\end{center}
\end{table}

\begin{table}[!h]
\begin{center}
\caption{Base de dados iris: Matriz de confusão obtida pelo algoritmo SOM adaptativo}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline
Classe/Cluster & 0,0 & 0,1 & 0,2 & 0,3 & 0,4 & 1,0 & 1,1 & 1,2 & 1,3 & 1,4 \\ \hline
1 & 0 & 0  & & & & & & & \\ \hline
2 & 0 & 0 & & & & & & & \\ \hline
3 & 9 & 25 & & & & & & & \\ \hline

%0,0 & 0 & 0 & 9 \\ \hline
%0,1 & 0 & 0 & 25 \\ \hline
%0,2 & 0 & 1 & 13 \\ \hline
%0,3 & 0 & 17 & 0 \\ \hline
%0,4 & 18 & 0 & 0 \\ \hline
%1,0 & 0 & 10 & 0 \\ \hline
%1,1 & 0 & 10 & 0 \\ \hline
%1,2 & 0 & 12 & 0 \\ \hline
%1,3 & 12 & 0 & 0 \\ \hline
%1,4 & 20 & 0 & 0\\ \hline

\end{tabular}
\label{iris_adaptativo}
\end{center}
\end{table}

Os índices CR, F-\textit{measure} e OERC obtiveram, respectivamente, os valores 0,46, 0,58, 0,027 para o modelo SOM adaptativo, e os valores, respectivamente 0,47, 0,68, 0,047 para o modelo SOM batch não-adaptativo.

\subsection{Base de dados Wine}

A base de dados wine consiste em resultados da análise química de vinhos produzidos numa mesma região na Itália, mas derivados de três cultivos diferentes, formando três classes de vinhos. As classes 1, 2 e 3 possuem, respectivamente 59, 71 e 48 instâncias. Cada indivíduo é descrito por 13 atributos com valores reais representando os valores de 13 componentes encontrados em cada um dos três tipos de vinho. Tais atributos são: (1) álcool; (2) ácido málico; (3) cinzas; (4) alcalinidade das cinzas; (5) magnésio; (6) fenóis totais; (7) flavonóides; (8) fenóis não-flavonóides; (9) proantocianinas; (10) intensidade da cor; (11) matiz; (12) OD280/OD315 de vinhos diluídos e (13) proline.

Os algoritmos SOM foram aplicados nesta base de dados com os parâmetros apresentados na tabela \ref{param_wine}. As tabelas \ref{wine_batch} e \ref{wine_adaptativo} mostram a matriz de confusão obtidas pelos algoritmos batch SOM e batch SOM adaptativo para múltiplas tabelas de dissimilaridade.

\begin{table}[!h]
\begin{center}
\caption{Parâmetros utilizados nos experimentos com a base wine}
\begin{tabular}{|c|c|}
\hline
Parâmetros & Valor\\
\hline
Topologia & 2x5\\ \hline
$T_{min}$ & 0,002\\ \hline
$T_{max}$ & 2,0\\ \hline
$N_{iter}$ & 300\\ \hline
\end{tabular}
\label{param_wine}
\end{center}
\end{table}

\begin{table}[!h]
\begin{center}
\caption{Base de dados wine: Matriz de confusão obtida pelo algoritmo batch SOM}
\begin{tabular}{|c|c|c|c|}
\hline
Cluster/Classe & 1 & 2 & 3 \\ \hline
0,0 & 33 & 1 & 0 \\ \hline
0,1 & 3 & 34 & 0 \\ \hline
0,2 & 0 & 22 & 0 \\ \hline
0,3 & 0 & 1 & 8 \\ \hline
0,4 & 0 & 0 & 26 \\ \hline
1,0 & 23 & 4 & 0 \\ \hline
1,3 & 0 & 5 & 2 \\ \hline
1,4 & 0 & 4 & 12\\ \hline

\end{tabular}
\label{wine_batch}
\end{center}
\end{table}

\begin{table}[!h]
\begin{center}
\caption{Base de dados wine: Matriz de confusão obtida pelo algoritmo batch SOM adaptativo}
\begin{tabular}{|c|c|c|c|}
\hline
Cluster/Classe & 1 & 2 & 3 \\ \hline
0,0 & 0 & 21 & 0 \\ \hline
0,1 & 0 & 9 & 0 \\ \hline
0,2 & 0 & 2 & 23 \\ \hline
0,3 & 8 & 0 & 0 \\ \hline
0,4 & 30 & 2 & 0 \\ \hline
1,0 & 1 & 16 & 0 \\ \hline
1,1 & 0 & 19 & 0 \\ \hline
1,2 & 2 & 0 & 0 \\ \hline
1,3 & 0 & 1 & 25 \\ \hline
1,4 & 18 & 1 & 0\\ \hline

\end{tabular}
\label{wine_adaptativo}
\end{center}
\end{table}

\section{Conclusão}

%\nocite{*} % displays all references, cited or not
\bibliographystyle{sbc}
\bibliography{dantas-DeCarvalho-enia}

\end{document}
